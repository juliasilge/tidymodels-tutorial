---
title: "Classwork 1"
subtitle: "Machine learning with tidymodels"
editor_options: 
  chunk_output_type: console
---

## Data on abalone ages

```{r}
library(tidymodels)
library(tidyverse)

url <- "https://bit.ly/tidymodels-abalone-csv"
abalone <- read_csv(url) %>% mutate_if(is.character, as.factor)
abalone
```

## Your turn

When is a good time to split your data?

## Data splitting and spending

```{r}
set.seed(123)
ring_split <- initial_split(abalone)
ring_split
```

Generate the training and testing sets:

```{r}
ring_train <- training(ring_split)
ring_test <- testing(ring_split)
```

## Your turn

Split your data so 20% is held out for the test set.

Try out different values in `set.seed()` to see how the results change.

Hint: Which argument in `initial_split()` handles the proportion split into training vs testing?

```{r}
# your code here!

```

## Your turn

Explore the `ring_train` data on your own!

- What's the distribution of the outcome, rings?

- What's the distribution of numeric variables like weight?

- How do rings differ across sex?

```{r}
# your code here!

```

## Stratification

```{r}
set.seed(123)
ring_split <- initial_split(abalone, prop = 0.8, strata = rings)
ring_train <- training(ring_split)
ring_test <- testing(ring_split)
```

## Your turn

- How do you fit a linear model in R?

- How many different ways can you think of?

- Discuss with your neighbor!

## To specify a model

```{r}
# model:
linear_reg()

# engine:
linear_reg() %>%
  set_engine("glmnet")

# mode - some models have a default mode, but others don't:
decision_tree() %>% 
  set_mode("regression")
```

## Your turn

Edit the chunk below to use a different model!

```{r tree_spec}
tree_spec <- decision_tree() %>% 
  set_mode("regression")

tree_spec
```

## A model workflow

```{r}
tree_spec <-
  decision_tree() %>% 
  set_mode("regression")
```

Fit the model:

```{r}
tree_spec %>% 
  fit(rings ~ ., data = ring_train) 
```

Fit with a workflow:

```{r}
workflow(rings ~ ., tree_spec) %>%
  fit(data = ring_train) 
```

## Your turn

This is how you'd fit a decision model:

```{r tree_wf}
tree_spec <- decision_tree(mode = "regression")
tree_wf <- workflow(rings ~ ., tree_spec)

fit(tree_wf, data = ring_train)
```

Now use a similar approach to fit a linear model! Call it `lm_wf`.

```{r}
# your code here!

```

## Predict with your model

```{r}
tree_spec <- decision_tree(mode = "regression")

tree_fit <-
  workflow(rings ~ ., tree_spec) %>% 
  fit(data = ring_train) 
```

## Your turn

What do you get from running the following code? What do you notice about the structure of the result?

```{r}
predict(tree_fit, new_data = ring_test)
```

## Your turn

What do you get from running the following code? How is `augment()` different from `predict()`?

```{r}
augment(tree_fit, new_data = ring_test)
```

## Understand your model

```{r}
library(rpart.plot)

tree_fit %>%
  extract_fit_engine() %>%
  rpart.plot(roundint = FALSE)
```

⚠️ Never `predict()` with any extracted components!

## Your turn

Explore how you might deploy your `tree_fit` model using vetiver:

```{r vetiver}
library(vetiver)
library(plumber)

# create a vetiver model object
v <- vetiver_model(tree_fit, "abalone-rings")
v
```

```{r}
# create a plumber API
pr <- pr() %>%
  vetiver_api(v)

pr
```

```{r}
# run the API server in a new window
pr_run(pr)
```


## Evaluating models

```{r}
tree_spec <- decision_tree(cost_complexity = 0.001, mode = "regression")
tree_wf <- workflow(rings ~ ., tree_spec)
tree_fit <- fit(tree_wf, ring_train)
```

## Metrics for model performance

`metrics()` returns a standard set of metrics:

```{r}
augment(tree_fit, new_data = ring_test) %>%
  metrics(rings, .pred)
```

Or you can use individual metric functions:

```{r}
augment(tree_fit, new_data = ring_test) %>%
  rmse(rings, .pred)
```

All yardstick metric functions work with grouped data frames!

```{r}
augment(tree_fit, new_data = ring_test) %>%
  group_by(sex) %>%
  rmse(rings, .pred)
```

Metric sets are a way to combine multiple metric functions:

```{r}
abalone_metrics <- metric_set(rmse, mape)

augment(tree_fit, new_data = ring_test) %>%
  abalone_metrics(rings, .pred)
```

## Dangers of overfitting

Repredicting the training set = bad!

```{r}
tree_fit %>%
  augment(ring_train)
```

"Resubstitution estimate" - This should be the best possible performance that you could ever achieve, but it can be very misleading!

```{r}
tree_fit %>%
  augment(ring_train) %>%
  rmse(rings, .pred)
```

Now on the test set, see that it performs worse? This is closer to "real" performance.

```{r}
tree_fit %>%
  augment(ring_test) %>%
  rmse(rings, .pred)
```

## Your turn

Use `augment()` and `metrics()` to compute a regression metric like `mae()`.

Compute the metrics for both training and testing data.

Notice the evidence of overfitting! ⚠️

```{r}
# your code here! use `augment()` and `metrics()` with `tree_fit`
tree_fit
```

## Your turn

If we use 10 folds, what percent of the training data:

- ends up in analysis?
- ends up in assessment?

for each fold

## Resampling

```{r}
# v = 10 is the default
vfold_cv(ring_train)
```

What is in a resampling result?

```{r}
ring_folds <- vfold_cv(ring_train, v = 10)

# Individual splits of analysis/assessment data
ring_folds$splits[1:3]
```

Stratification often helps, with very little downside

```{r}
vfold_cv(ring_train, strata = rings)
```

We'll use this setup:

```{r}
set.seed(234)
ring_folds <- vfold_cv(ring_train, v = 5, strata = rings)
ring_folds
```

## Evaluating model performance

```{r}
# fit the workflow on each analysis set,
# then compute performance on each assessment set
tree_res <- fit_resamples(tree_wf, ring_folds)
tree_res
```

Aggregate metrics:

```{r}
tree_res %>%
  collect_metrics()
```

If you want to analyze the assessment set (i.e. holdout) predictions, then you need to adjust the control object and tell it to save them:

```{r}
# save the assessment set results
ctrl_abalone <- control_resamples(save_pred = TRUE)
tree_res <- fit_resamples(tree_wf, ring_folds, control = ctrl_abalone)

tree_preds <- collect_predictions(tree_res)
tree_preds
```

```{r}
tree_preds %>% 
  ggplot(aes(rings, .pred, color = id)) + 
  geom_abline(lty = 2, col = "gray", size = 1.5) +
  geom_point(alpha = 0.5) +
  coord_obs_pred()
```

## Bootstrapping

```{r}
set.seed(3214)
bootstraps(ring_train)
```

## Your turn

Create:

- Bootstrap folds (change `times` from its default!)
- A validation resample (what function is used for this?)

<https://rsample.tidymodels.org/reference/>

Don't forget to set a seed when you resample!

```{r}
# your code here!

```

## Create a random forest model

```{r}
rf_spec <- rand_forest(trees = 1000, mode = "regression")
rf_spec
```

```{r}
rf_wf <- workflow(rings ~ ., rf_spec)
rf_wf
```

## Your turn

Use `fit_resamples()` and `rf_wf` to:

- Keep predictions
- Compute metrics
- Plot true vs predicted values

```{r}
# your code here!

```

## Evaluate a workflow set

```{r}
wf_set <- workflow_set(list(rings ~ .), list(tree_spec, rf_spec))
wf_set
```

```{r}
wf_set_fit <- wf_set %>%
  workflow_map("fit_resamples", resamples = ring_folds)

wf_set_fit
```

Rank the sets of models by their aggregate metric performance

```{r}
wf_set_fit %>%
  rank_results()
```

## Your turn

When do you think a workflow set would be useful?

Discuss with your neighbors!

## The final fit

```{r}
# `ring_split` has train + test info
final_fit <- last_fit(rf_wf, ring_split) 

final_fit
```

Test set metrics:

```{r}
collect_metrics(final_fit)
```

Test set predictions:

```{r}
collect_predictions(final_fit)
```

```{r}
collect_predictions(final_fit) %>%
  ggplot(aes(rings, .pred)) + 
  geom_abline(lty = 2, col = "deeppink4", size = 1.5) +
  geom_point(alpha = 0.5) +
  coord_obs_pred()
```

```{r}
extract_workflow(final_fit)
```

## Your turn

Which model do you think you would decide to use?

What surprised you the most?

What is one thing you are looking forward to next?
